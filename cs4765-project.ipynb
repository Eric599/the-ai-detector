{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3bae57-d36d-4322-9707-e0b45045a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to load data and visualize\n",
    "# use longformer tokenizer (efficient at tokenizing large texts)\n",
    "# Set Up the Model for Fine-Tuning (transition to Colab here?)\n",
    "# Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c448fc0d-891e-45b8-a353-363dd161edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label   model   source  \\\n",
      "0  Giving gifts should always be enjoyable.  Howe...      1  bloomz  wikihow   \n",
      "1  Yveltal (Japanese: ユベルタル) is one of the main a...      1  bloomz  wikihow   \n",
      "2  If you'd rather not annoy others by being rude...      1  bloomz  wikihow   \n",
      "3  If you're interested in visiting gravesite(s) ...      1  bloomz  wikihow   \n",
      "4  The following are some tips for becoming succe...      1  bloomz  wikihow   \n",
      "\n",
      "   id  \n",
      "0   0  \n",
      "1   1  \n",
      "2   2  \n",
      "3   3  \n",
      "4   4  \n"
     ]
    }
   ],
   "source": [
    "# Visualizing the data with pandasimport pandas as pd\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_json('./datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d6465a4-a99a-4070-ba19-43c568b25b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer function\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
    "# #print(tokenizer(\"Hello world\")[\"input_ids\"])\n",
    "# print(tokenizer('./datasets/subtaskA_dev_monlingual.jsonl')[\"input_ids\"])\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f501f194-2993-4e70-9352-30ae0f3afb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:06<00:00, 731.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'model', 'source', 'id', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc310481-09f3-4955-bc27-3ae402c0d512",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## test tokenizer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/subtaskA_dev_monlingual.jsonl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m, in \u001b[0;36mtokenize_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae077cc-2c3d-4714-9279-5b995263108b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
